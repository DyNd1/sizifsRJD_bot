{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gXBeXrzNDE7b"},"outputs":[],"source":["#!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5W2tf1QFFVQ"},"outputs":[],"source":["pip install transformers torch peft"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1699759622233,"user":{"displayName":"Илья Аверьянов","userId":"04003119781328550623"},"user_tz":-420},"id":"T4XXfhFpJkaP","outputId":"764a82be-e486-4f9f-b491-fc1c1f685c75"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283,"referenced_widgets":["73ef30f9c85d4665b4b1e2de9d02973c","780e5954720143eb97e5d4c0fad464c4","f6debee362c042918fc5bc2152bce7d9","3b1fad843f84447b8f3b3961897a3c16","dc5db3afca474e32bf942e55a8e0a01a","1af8831db5f242f996434203c1e0266e","0c47ce6860434509bf3d7e9e4a25cba2","e40c0670061b48d9a9d22bf4432c1651","b712f2c341bb44ec96cb2dee6bd59670","5295562898f94adc93baa38684ba7d81","1453cb0918cf402fb6ab5d8388154736"]},"executionInfo":{"elapsed":170161,"status":"ok","timestamp":1699761922796,"user":{"displayName":"Илья Аверьянов","userId":"04003119781328550623"},"user_tz":-420},"id":"gcCZmFhNG35G","outputId":"b6b153a0-b8b3-4605-db06-3a1ec043ca7d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/34 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ef30f9c85d4665b4b1e2de9d02973c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:527: UserWarning: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration. This warning will be raised to an exception in v4.34.\n","\n","Thrown during validation:\n","`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["('/content/sizifs-llma-v1.4/tokenizer_config.json',\n"," '/content/sizifs-llma-v1.4/special_tokens_map.json',\n"," '/content/sizifs-llma-v1.4/tokenizer.json')"]},"metadata":{},"execution_count":1}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import transformers\n","import torch\n","from peft import AutoPeftModelForCausalLM, PeftModel\n","\n","tokenizer = AutoTokenizer.from_pretrained('sharpbai/Llama-2-7b-hf')\n","base_model_name = \"sharpbai/Llama-2-7b-hf\" # change for your basemodel\n","\n","\n","new_model_name = \"/content/sizifs-llma-v1.3\"\n","model_path = \"/content/sizifs-llma-v1.4\"\n","\n","\n","device_map = {\"\": 0} # single gpu?\n","\n","base_model = AutoModelForCausalLM.from_pretrained(\n","base_model_name,\n","low_cpu_mem_usage=True,\n","return_dict=True,\n","torch_dtype=torch.float16,\n","device_map=device_map,\n","offload_folder=\"offload\",\n","offload_state_dict = True\n",")\n","model = PeftModel.from_pretrained(\n","    base_model,\n","    new_model_name,\n","    use_ram_optimized_load=False,\n","    offload_folder=\"offload\")\n","merged_model = model.merge_and_unload()\n","\n","\n","merged_model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJXznIg4UfP0"},"outputs":[],"source":["!cp -r \"/content/rleise-work-sizifs-1\" \"/content/drive/MyDrive/model_sizifs\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"BKL7UtgzvAHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","source_path = '/content/sizifs-llma-v1.4/model-00001-of-00003.safetensors'\n","destination_path = '/content/drive/MyDrive/sizifs-llma-v1.4/model-00001-of-00003.safetensors'\n","\n","!cp $source_path $destination_path"],"metadata":{"id":"srsccJkSvFMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epf88mfgVuXd"},"outputs":[],"source":["!cp \"/content/rleise-work-sizifs-1/pytorch_model-00002-of-00002.bin\" \"/content/drive/MyDrive/model_sizifs\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hN_4g3euX4or"},"outputs":[],"source":["#model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True,  use_auth_token=access_token)\n","#tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)\n","model_inputs = tokenizer(\"Зачем нужны ПТЭ?\", return_tensors=\"pt\").to(\"cuda:0\")\n","\n","output = merged_model.generate(**model_inputs)\n","\n","print(tokenizer.decode(output[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JoP40xtmZrbi"},"outputs":[],"source":["output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXxN0rdqaFel"},"outputs":[],"source":["model_inputs = tokenizer(\"ЧТо такое роль\", return_tensors=\"pt\").to(\"cuda:0\")\n","\n","output = model.generate(**model_inputs)\n","\n","print(tokenizer.decode(output[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmwhReiaaoJE"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM,LlamaTokenizer\n","import transformers\n","import torch\n","from peft import AutoPeftModelForCausalLM, PeftModel\n","\n","\n","\n","device_map = {\"\": 0} # single gpu?\n","\n","model = AutoModelForCausalLM.from_pretrained(\"/content/sizifs-llma-v1.4\",\n","                                             low_cpu_mem_usage=True,\n","return_dict=True,\n","torch_dtype=torch.float16,\n","device_map=device_map,\n","offload_folder=\"/content/sizifs-llma-v1.4\",\n","offload_state_dict = False)\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/sizifs-llma-v1.4\")\n","\n","#output = model.generate(**model_inputs)\n","user_input = \"расскажи про внутренние правила\"\n","promt = \"User: {}\\nAI:\".format(user_input)\n","input_ids = tokenizer.encode(promt, return_tensors=\"pt\").to(\"cuda:0\")\n","output = model.generate(input_ids, max_length=512, num_return_sequences=1,temperature=0.7 )\n","\n","print(tokenizer.decode(output[0], skip_special_tokens=True))"]},{"cell_type":"code","source":["\n","\n","#output = model.generate(**model_inputs)\n","user_input = \"Когда я уже схожу покакать?\"\n","promt = \"User: {}\\nAI:\".format(user_input)\n","input_ids = tokenizer.encode(promt, return_tensors=\"pt\").to(\"cuda:0\")\n","output = model.generate(input_ids, max_length=512, num_return_sequences=1 )\n","\n","out_str = str(tokenizer.decode(output[0], skip_special_tokens=True)).split(\"\\n\")[1][4:]\n","\n","\n","print(out_str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ammxmk2buReX","executionInfo":{"status":"ok","timestamp":1699763785945,"user_tz":-420,"elapsed":17449,"user":{"displayName":"Илья Аверьянов","userId":"04003119781328550623"}},"outputId":"8770b3f1-5b65-430d-f368-8ba5a760e71f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["На участках, оборудованных автоблокировкой, запрещается движение поездов при показании двух желтых огней накрайнем мигающем огне путевого светофора и одном желтым огне путевого светофора при машинисте, увидевшем сигнал локомотива, показывающий красный огонь. На участках с автоблокировкой запрещается движение поездов, если на красных сигналах двух желтых огней на крайнем мигающем огне и один желтый сигнал, если машинист видит сигнал локомотива с красным огнем.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"je43ESotY20j"},"outputs":[],"source":["output"]},{"cell_type":"code","source":[],"metadata":{"id":"26Oka_ket1dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wbtNifUJUBM"},"outputs":[],"source":["import torch\n","from transformers import AutoConfig\n","\n","# Prompt user for input paths\n","config_path = input(\"Enter the path to the config.json file: \")\n","model1_path = input(\"Enter the path to the first model .bin file (model1.bin): \")\n","model2_path = input(\"Enter the path to the second model .bin file (model2.bin): \")\n","\n","# Load the configuration for the model you are working with\n","config = AutoConfig.from_pretrained(config_path)\n","\n","# Load the two separate model .bin files\n","model_1 = torch.load(model1_path)\n","model_2 = torch.load(model2_path)\n","\n","# Merge the state dictionaries of the two models\n","merged_state_dict = model_1.copy()\n","for key, value in model_2.items():\n","    if key not in merged_state_dict:\n","        merged_state_dict[key] = value\n","    else:\n","        # If the keys already exist in the first model, average the values\n","        merged_state_dict[key] = (merged_state_dict[key] + value) / 2\n","\n","# Save the merged model\n","torch.save(merged_state_dict, \"pytorch_model.bin\")\n","\n","print(\"Merged model saved as pytorch_model.bin\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9bTPueZDZBP"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import transformers\n","import torch\n","from peft import AutoPeftModelForCausalLM, PeftModel\n","\n","tokenizer = AutoTokenizer.from_pretrained('/llama-2-7b-chat.ggmlv3.q4_1')\n","\n","\n","base_model_name = \"llama-2-7b-chat.ggmlv3.q4_1\"\n","base_model = AutoModelForCausalLM.from_pretrained(\n","base_model_name,\n","low_cpu_mem_usage=True,\n","return_dict=True,\n","torch_dtype=torch.float32,\n","device_map=\"auto\",\n","offload_folder=\"offload\",\n","offload_state_dict = True\n",")\n","\n","\n","new_model_name = \"/rleise-work-sizifs-1\"\n","\n","model = PeftModel.from_pretrained(base_model, new_model_name,use_ram_optimized_load=False,offload_folder=\"offload\")\n","merged_model = model.merge_and_unload()\n","\n","#Save the merged model\n","merged_model.save_pretrained(\"merged_model\",safe_serialization=True,device_map=\"auto\")\n","tokenizer.save_pretrained(\"merged_model\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1kT0DM50J0fSoEaAOnnN4ul6UEAczge1F","timestamp":1699719409245}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"73ef30f9c85d4665b4b1e2de9d02973c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_780e5954720143eb97e5d4c0fad464c4","IPY_MODEL_f6debee362c042918fc5bc2152bce7d9","IPY_MODEL_3b1fad843f84447b8f3b3961897a3c16"],"layout":"IPY_MODEL_dc5db3afca474e32bf942e55a8e0a01a"}},"780e5954720143eb97e5d4c0fad464c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1af8831db5f242f996434203c1e0266e","placeholder":"​","style":"IPY_MODEL_0c47ce6860434509bf3d7e9e4a25cba2","value":"Loading checkpoint shards: 100%"}},"f6debee362c042918fc5bc2152bce7d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e40c0670061b48d9a9d22bf4432c1651","max":34,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b712f2c341bb44ec96cb2dee6bd59670","value":34}},"3b1fad843f84447b8f3b3961897a3c16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5295562898f94adc93baa38684ba7d81","placeholder":"​","style":"IPY_MODEL_1453cb0918cf402fb6ab5d8388154736","value":" 34/34 [01:03&lt;00:00,  1.63s/it]"}},"dc5db3afca474e32bf942e55a8e0a01a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1af8831db5f242f996434203c1e0266e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c47ce6860434509bf3d7e9e4a25cba2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e40c0670061b48d9a9d22bf4432c1651":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b712f2c341bb44ec96cb2dee6bd59670":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5295562898f94adc93baa38684ba7d81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1453cb0918cf402fb6ab5d8388154736":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}